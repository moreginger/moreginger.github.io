---
layout: post
title: When hashCodes of Sets go bad
---

Had fun today with what looked like perfectly good Java hashCodes, generated by
Eclipse. These typically take a form like:
```
public int hashCode() {
  final int prime = 31;
  int result = 1;
  result = prime * result + _field1.hashCode();
  result = prime * result + _field2.hashCode();
  return result;
}
```
This is a decent hashcode and fast to compute. The issue we encountered was when
placing these objects in Sets. For example, we have two Sets of two Tuple objects
like so:
```
final Set<Tuple> set1 = ImmutableSet.of(new Tuple(1, 1), new Tuple(2, 2));
final Set<Tuple> set2 = ImmutableSet.of(new Tuple(1, 2), new Tuple(2, 1));
```
The thing about the hashCode of a Set is that, for obvious reasons, it is order
independent. In fact it just sums up the hashCodes of the elements in the Set,
and the hashCodes we have are.

+ Tuple(1, 1): (31 + 1) * 31 + 1 = 993
+ Tuple(2, 2): (31 + 2) * 31 + 2 = 1025
+ Tuple(1, 2): (31 + 1) * 31 + 2 = 994
+ Tuple(2, 1): (31 + 2) * 31 + 1 = 1024

Whoops, we now get the same hashCode for both Sets. Now, we might expect that
real data isn't likely to follow this pattern. Test data on the other hand
might. And indeed this is what we have, resulting in data that causes a vastly
elevated number of hash collisions, putting the equals implementation under
heavy strain.

My esteemed colleague Chris suggested using instead the Cantor hashCode to stop
the operations commuting. Indeed this does solve the problem, but to which
implementations of hashCode should we apply the new (slower) implementation? All
of them? Or
would it be better to try and improve the realism of the test data? Of course,
a client might be expected to generate test data with exactly the same issue,
so there is a strong argument for working around the problem if it is expected
to be an issue.
